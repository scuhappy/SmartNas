import os
import re
import json
import asyncio
import requests
from urllib.parse import quote
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# è¯»å–é…ç½®æ–‡ä»¶
def load_config():
    """è¯»å–cover_config.jsoné…ç½®æ–‡ä»¶"""
    try:
        with open('cover_config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âŒ cover_config.json æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return {
            "video_paths": ["/media"],
            "cover_path": "./covers",
            "meta_config": "metadata.json"
        }

# åŠ è½½é…ç½®
CONFIG = load_config()
BASE_URL = "https://javday.app"

async def search_javday(fanhao: str):
    """ä½¿ç”¨ Playwright è®¿é—® javday æœç´¢å¹¶è§£æç»“æœ"""
    url = f"{BASE_URL}/search?wd={quote(fanhao)}"
    results = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Referer": "https://javday.app/",
                "Connection": "keep-alive",
            },
            ignore_https_errors=True
        )
        page = await context.new_page()

        try:
            print(f"ğŸ” æœç´¢ {fanhao}: {url}")
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await page.wait_for_selector(".videoBox", timeout=15000)
            content = await page.content()

            soup = BeautifulSoup(content, "html.parser")
            for box in soup.select(".videoBox"):
                cover_div = box.select_one(".videoBox-cover")
                cover_url = None
                if cover_div and "style" in cover_div.attrs:
                    m = re.search(r"url\((.*?)\)", cover_div["style"])
                    if m:
                        cover_url = BASE_URL + m.group(1)

                title_span = box.select_one(".videoBox-info .title")
                title = title_span.get_text(strip=True) if title_span else None

                if cover_url and title:
                    results.append({"title": title, "cover": cover_url})

        except Exception as e:
            print(f"âŒ æœç´¢ {fanhao} å¤±è´¥: {e}")
        finally:
            await browser.close()

    return results

async def download_cover(url, fanhao, filename, save_dir, retries=3):
    """ä¸‹è½½å°é¢å¹¶ä»¥è§†é¢‘æ–‡ä»¶åå‘½åï¼Œæ”¯æŒé‡è¯•å’Œå¤‡ç”¨ä¸‹è½½"""
    os.makedirs(save_dir, exist_ok=True)
    save_name = os.path.splitext(filename)[0]
    save_path = os.path.join(save_dir, f"{save_name}.jpg")

    # æ£€æŸ¥å°é¢æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(save_path):
        print(f"âœ… {fanhao} å°é¢å·²å­˜åœ¨: {save_path}")
        return save_path

    # å°è¯•ä½¿ç”¨ Playwright ä¸‹è½½
    for attempt in range(retries):
        try:
            print(f"ğŸ“¥ å°è¯•ä¸‹è½½ {fanhao} å°é¢ (ç¬¬ {attempt + 1}/{retries}): {url}")
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(ignore_https_errors=True)
                page = await context.new_page()
                response = await page.goto(url, wait_until="networkidle", timeout=30000)
                if response and response.status == 200:
                    content = await response.body()
                    with open(save_path, "wb") as f:
                        f.write(content)
                    await browser.close()
                    print(f"âœ… {fanhao} å°é¢å·²ä¿å­˜: {save_path}")
                    return save_path
                else:
                    print(f"âš  {fanhao} å°é¢ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status if response else 'æ— å“åº”'}")
                    await browser.close()
        except Exception as e:
            print(f"âŒ {fanhao} å°é¢ä¸‹è½½å¤±è´¥ (ç¬¬ {attempt + 1}/{retries}): {e}")

    # å¤‡ç”¨ä¸‹è½½ï¼šä½¿ç”¨ requests
    print(f"ğŸ”„ å°è¯•ä½¿ç”¨ requests ä¸‹è½½ {fanhao} å°é¢: {url}")
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://javday.app/",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept": "image/avif,image/webp,image/*,*/*;q=0.8",
        }
        response = requests.get(url, headers=headers, timeout=30, verify=False)
        response.raise_for_status()
        with open(save_path, "wb") as f:
            f.write(response.content)
        print(f"âœ… {fanhao} å°é¢å·²ä¿å­˜ (requests): {save_path}")
        return save_path
    except Exception as e:
        print(f"âŒ {fanhao} å°é¢ä¸‹è½½å¤±è´¥ (requests): {e}")
        return None

def extract_fanhao(filename):
    """ä»æ–‡ä»¶åä¸­æå–ç•ªå·"""
    match = re.search(r"[A-Z]{2,5}-\d{2,5}", filename, re.I)
    return match.group(0).upper() if match else None

def get_relative_cover_path(cover_path, base_path):
    """è®¡ç®—å°é¢ç›¸å¯¹äºè„šæœ¬è¿è¡Œç›®å½•çš„è·¯å¾„"""
    try:
        return os.path.relpath(cover_path, os.getcwd()).replace(os.sep, '/')
    except ValueError:
        # è·¨åˆ†åŒºæ—¶ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„å¹¶è§„èŒƒåŒ–
        print(f"âš  è·¨åˆ†åŒºè·¯å¾„: {cover_path} (åŸºäº {base_path})")
        return os.path.abspath(cover_path).replace(os.sep, '/')

def extract_actor_name(title):
    """ä»æ ‡é¢˜ä¸­æå–æ¼”å‘˜åå­—ï¼Œå–æœ€åä¸€ä¸ªç©ºæ ¼å­—ç¬¦åé¢çš„å†…å®¹"""
    if not title:
        return None
    
    # æŸ¥æ‰¾æœ€åä¸€ä¸ªç©ºæ ¼
    last_space_index = title.rfind(' ')
    if last_space_index == -1:
        return None
    
    # æå–æœ€åä¸€ä¸ªç©ºæ ¼åçš„å†…å®¹ä½œä¸ºæ¼”å‘˜åå­—
    actor_name = title[last_space_index + 1:].strip()
    return actor_name if actor_name else None

async def process_videos(folder_path, cover_path, json_file="metadata.json"):
    """é€’å½’éå†æ–‡ä»¶å¤¹åŠå…¶å­æ–‡ä»¶å¤¹ï¼Œæå–ç•ªå·ï¼Œä¸‹è½½å°é¢å¹¶ä¿å­˜å…ƒæ•°æ®"""
    video_extensions = ('.mp4', '.mkv', '.avi', '.mov', '.wmv')
    metadata = load_existing_metadata(json_file)
    record_count = 0
    base_path = os.path.abspath(folder_path)

    for root, _, files in os.walk(folder_path):
        for filename in files:
            if filename.lower().endswith(video_extensions):
                fanhao = extract_fanhao(filename)
                if not fanhao:
                    print(f"âš  {filename} æœªæ‰¾åˆ°æœ‰æ•ˆç•ªå·")
                    continue

                print(f"å¤„ç†æ–‡ä»¶: {os.path.join(root, filename)} (ç•ªå·: {fanhao})")
                # è·³è¿‡å·²å­˜åœ¨äº metadata çš„ç•ªå·
                if fanhao in metadata:
                    print(f"âœ… {fanhao} å·²å­˜åœ¨äºå…ƒæ•°æ®ï¼Œè·³è¿‡")
                    continue

                items = await search_javday(fanhao)
                if not items:
                    print(f"âš  {fanhao} æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
                    continue

                # åªå¤„ç†ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
                item = items[0]
                downloaded_cover_path = await download_cover(item["cover"], fanhao, filename, cover_path)
                if downloaded_cover_path:
                    # å°é¢è·¯å¾„ï¼šç›¸å¯¹äºè„šæœ¬è¿è¡Œç›®å½•
                    relative_cover_path = get_relative_cover_path(downloaded_cover_path, base_path)
                    # è§†é¢‘è·¯å¾„ï¼šç»å¯¹è·¯å¾„ï¼Œè§„èŒƒåŒ–æ–œæ 
                    absolute_video_path = os.path.abspath(os.path.join(root, filename)).replace(os.sep, '/')
                    # æå–æ¼”å‘˜åå­—
                    actor_name = extract_actor_name(item["title"])
                    metadata[fanhao] = {
                        "title": item["title"],
                        "cover_path": relative_cover_path,
                        "video_file": absolute_video_path,
                        "actor_name": actor_name
                    }
                    record_count += 1

                    # æ¯ 10 æ¡è®°å½•å†™å…¥ä¸€æ¬¡
                    if record_count >= 10:
                        with open(json_file, "w", encoding="utf-8") as f:
                            json.dump(metadata, f, ensure_ascii=False, indent=4)
                        print(f"ğŸ“ å·²ä¿å­˜ {record_count} æ¡å…ƒæ•°æ®è‡³: {json_file}")
                        record_count = 0

    # ä¿å­˜å‰©ä½™çš„å…ƒæ•°æ®
    if record_count > 0:
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        print(f"ğŸ“ æœ€ç»ˆä¿å­˜ {record_count} æ¡å…ƒæ•°æ®è‡³: {json_file}")

def load_existing_metadata(json_file):
    """åŠ è½½ç°æœ‰çš„ metadata.json æ–‡ä»¶"""
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

async def main():
    """ä¸»å‡½æ•°ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„"""
    video_paths = CONFIG.get("video_paths", ["/media"])
    cover_path = CONFIG.get("cover_path", "./covers")
    meta_config = CONFIG.get("meta_config", "metadata.json")
    
    # ç¡®ä¿å°é¢ç›®å½•å­˜åœ¨
    os.makedirs(cover_path, exist_ok=True)
    
    print(f"ğŸ“ è§†é¢‘è·¯å¾„: {video_paths}")
    print(f"ğŸ“ å°é¢ä¿å­˜è·¯å¾„: {cover_path}")
    print(f"ğŸ“ å…ƒæ•°æ®æ–‡ä»¶: {meta_config}")
    
    total_processed = 0
    
    # å¤„ç†æ‰€æœ‰è§†é¢‘è·¯å¾„
    for video_path in video_paths:
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘è·¯å¾„ä¸å­˜åœ¨: {video_path}")
            continue
            
        print(f"ğŸ”„ å¼€å§‹å¤„ç†è·¯å¾„: {video_path}")
        await process_videos(video_path, cover_path, meta_config)
        total_processed += 1
    
    print(f"âœ… æ‰€æœ‰è·¯å¾„å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {total_processed} ä¸ªè·¯å¾„")

if __name__ == "__main__":
    asyncio.run(main())